{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from statsmodels.tsa.stattools import pacf, acf\n",
    "from statsmodels.graphics import tsaplots as tsp\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "from sklearn import mixture as mxt\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from scipy import stats as scpSta\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import ffn\n",
    "import pyfolio as pf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the returns in % for the daily adjusted close. Later on we can look at the Drawdown based on the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock(indx,start,end):\n",
    "    \n",
    "        dataF = pdr.get_data_tiingo(indx, start, end, api_key='b7a9706daf700d3eafed86f85a5ff0273864f149')['adjClose'].reset_index(0,drop=True)\n",
    "    \n",
    "        return dataF\n",
    "\n",
    "    \n",
    "indx = ['SPY', 'QQQ', 'TLT', 'GLD', 'EFA', 'EEM']\n",
    "\n",
    "end = pd.to_datetime('2018-12-31')\n",
    "start = end - 20 * 252 * pd.tseries.offsets.BDay()\n",
    "\n",
    "\n",
    "    \n",
    "prices_df = (pd.DataFrame.from_dict({sym: get_stock(sym,start,end) for sym in indx}))\n",
    "\n",
    "prices_df.dropna(inplace=True)\n",
    "\n",
    "prices_df = prices_df.loc[~(prices_df==0).all(axis=1)]\n",
    "\n",
    "prices_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the overall % change in returns we can see the evident crash of 2008\n",
    "R0 = (prices_df.pct_change()).dropna()\n",
    "\n",
    "for cols in R0.columns:\n",
    "    fig, ax0 = plt.subplots(figsize=(20,10))\n",
    "    R0[cols].plot(ax=ax0,marker='o', ms=5.,markeredgecolor='white', markeredgewidth=.1,title=cols)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R0['year'] = R0.index.year\n",
    "def plotStd(data):\n",
    "    fig, (ax1,ax2) = plt.subplots(figsize=(15,10),nrows=2)\n",
    "    ax1.plot(data.groupby(data['year']).std(),marker=\"o\",label='Std Dev')\n",
    "    ax2.plot(data.groupby(data['year']).mean(),marker=\"o\",label='Mean')\n",
    "    ax1.legend(data)\n",
    "    ax2.legend(data)\n",
    "\n",
    "for cols in R0.columns:\n",
    "        adf_test = adfuller(R0[cols])\n",
    "        print(\"ADF = \" + str(adf_test[0]))\n",
    "        print(\"p-value = \" + str('%.5f'%adf_test[1]))\n",
    "    \n",
    "plotStd(R0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the columns stationary for normality constraint\n",
    "R = R0[:-1].diff(periods=1)\n",
    "R.dropna(inplace=True)\n",
    "R['year'] = R.index.year\n",
    "\n",
    "plotStd(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysing the data distribution for the return indices yearwise\n",
    "for cols in R.columns[:-1]:\n",
    "    fig, ax = plt.subplots(figsize=(7,6.5))\n",
    "    for year in R['year'].unique():    \n",
    "        sns.kdeplot(R[cols][R['year']==year],shade=True,label=str(cols) + '--' + str(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the unit root with Dicky Fuller and correlation test\n",
    "for cols in R.columns:\n",
    "        adf_test = adfuller(R[cols])\n",
    "        print(\"ADF = \" + str(adf_test[0]))\n",
    "        print(\"p-value = \" + str('%.5f'%adf_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.lag_plot(R['SPY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Partial autocorrelation and autocorrelation\n",
    "def plot_pacf_acf(R0,lag0,alpha0):\n",
    "    fig,(ax3,ax4,ax5,ax6) = plt.subplots(figsize=(15,12),nrows=4)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "    R0.plot(ax=ax3, marker='o', c='gray', ms=5.,markeredgecolor='white', markeredgewidth=.1)   \n",
    "    tsp.plot_pacf(R0,alpha=alpha0,lags=lag0,ax=ax4)\n",
    "    tsp.plot_acf(R0,alpha=alpha0,lags=lag0,ax=ax5)\n",
    "    sm.qqplot(R0, line='s', marker='o',markeredgewidth=0.2,ax=ax6)\n",
    "\n",
    "for year in R['year'].unique()[1:]:    \n",
    "    plot_pacf_acf(R[R['year']==year]['SPY'], lag0=120, alpha0=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Although we have used the differencing methods we can see that PACF and ACF for each year are having dependencies (showing the underlying correlation in the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a solution, testing the train and test splits in the dataset for checking the ideal split where test and train splits are both from the same data distribution\n",
    "train_size = [252, 252*2, 252*3, 252*4] ##252 = Busi Days in one year\n",
    "folds = 4\n",
    "Rdata = R['SPY']['2005':]\n",
    "\n",
    "gs = gridspec.GridSpec(folds, len(train_size), wspace=0.0)\n",
    "\n",
    "fig = plt.figure(figsize=(20,25))\n",
    "\n",
    "i = 0\n",
    "dataRaw = []\n",
    "\n",
    "for size in train_size:\n",
    "    tsobj = TimeSeriesSplit(n_splits=folds, max_train_size=size)\n",
    "    j = 0\n",
    "        \n",
    "    for train_index, test_index in tsobj.split(Rdata):\n",
    "        \n",
    "        Rdata_train = Rdata[train_index] \n",
    "        Rdata_test = Rdata[test_index]\n",
    "        \n",
    "        score,p = ks_2samp(Rdata_train, Rdata_test)\n",
    "        ax1 = plt.subplot(gs[i,j])\n",
    "        \n",
    "        sns.kdeplot(Rdata_train,shade=True,label=str('Train') + '-- KS Test -- ' + str('%.5f'%score),ax=ax1)\n",
    "        sns.kdeplot(Rdata_test,shade=True,label=str('Test') + '-- P value --' + str('%.6f'%p),ax=ax1)\n",
    "        plt.title('Start : ' + str(Rdata_train.index[0]) + '\\n End :' + str(Rdata_train.index[-1]))\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.subplots_adjust(top=1.00)\n",
    "        \n",
    "        j = j + 1\n",
    "        \n",
    "        \n",
    "        row = (size, score, p , Rdata_train.index[0],Rdata_train.index[-1],Rdata_test.index[0],Rdata_test.index[-1])\n",
    "        dataRaw.append(row)\n",
    "\n",
    " \n",
    "    i = i + 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['train_size','KS','p-value','train_startD','train_endD','test_startD','test_endD']\n",
    "dataSplits = pd.DataFrame(dataRaw, columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pThreshold = 0.01\n",
    "dataSplitAcpt = dataSplits[dataSplits['p-value']<pThreshold]\n",
    "\n",
    "len(dataSplitAcpt)/len(dataSplits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The percentage of Train/Test splits which accept the null hypothesis that Test and Train came from the same distribution is 43% for p value of 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,row in dataSplitAcpt.iterrows():\n",
    "    print(str(row['train_startD']) + ' to ')\n",
    "    plot_pacf_acf(R['SPY'][pd.to_datetime(row['train_startD']) : pd.to_datetime(row['test_endD'])],lag0=60, alpha0=0.5)\n",
    "    print(str(row['test_endD']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data comes from different distributions, from different market segments over the years. Thus, understanding the underlying distributions in the data for better predictions of the returns and drawdowns from the individual clustered normal distributions. \n",
    "\n",
    "Now applying the Gaussian Mixture Models for cluster estimation on the SPY data:\n",
    "1. The GMM uses unsupervised approach to cluster the data. We first try to understand the optimal number of components ( different distribution to assume in the Joint distribution of data)\n",
    "2. AIC and BIC referenced to Akaike Information Criterion and the Bayesian Information Criterion are used to compare the results of 2 or models based on the overfitting nature of the model. The penalty is baed on the number of parameters (Components used in model) used for the model training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CpyRdata = Rdata\n",
    "CpyRdata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nCpyRdata = np.asarray(CpyRdata) \n",
    "nCpyRdata = nCpyRdata.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gmm(n_components, max_iter, R):\n",
    "    \n",
    "    GMM = mxt.GaussianMixture(n_components, covariance_type='full', random_state=R, tol=0.001, \n",
    "                              reg_covar=1e-06, max_iter=max_iter, n_init=1)\n",
    "    return GMM\n",
    "\n",
    "def print_gmm_results(gmm, X):\n",
    "    print('-'*25)\n",
    "    print(f'means: {gmm.means_.ravel()}')\n",
    "    print('-'*25)\n",
    "    print(f'covars: {gmm.covariances_.ravel()}')\n",
    "    print('-'*25)\n",
    "    print(f'sqrt covar: {np.sqrt(gmm.covariances_.ravel())}')\n",
    "    print('-'*25)\n",
    "    print(f'aic: {gmm.aic(X):.5f}')\n",
    "    print(f'bic: {gmm.bic(X):.5f}')\n",
    "    print('-'*25)\n",
    "\n",
    "GMM = create_gmm(5,150,0)\n",
    "GMM = GMM.fit(nCpyRdata)\n",
    "print_gmm_results(GMM, nCpyRdata)\n",
    "\n",
    "labels = GMM.predict(nCpyRdata)\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.scatter(CpyRdata.index, nCpyRdata[:, 0], c=labels, s=5, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAICBIC(X, minComponents, maxComponents):\n",
    "\n",
    "    dataRaw = []\n",
    "    for i in range(minComponents, maxComponents):\n",
    "\n",
    "        GMM = create_gmm(i,150,0)\n",
    "        GMM = GMM.fit(nCpyRdata)\n",
    "        row = (i, GMM.aic(X), GMM.bic(X))\n",
    "        dataRaw.append(row)\n",
    "    \n",
    "    columns = ['index','AIC','BIC']\n",
    "    AicDf = pd.DataFrame(dataRaw, columns = columns)    \n",
    "    AicDf = AicDf.set_index('index')\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "    sns.lineplot(data=AicDf)\n",
    "    \n",
    "    aicMinIn = AicDf[AicDf['AIC']==AicDf['AIC'].min()].index\n",
    "    bicMinIn = AicDf[AicDf['BIC']==AicDf['BIC'].min()].index\n",
    "    \n",
    "    plt.axvline(aicMinIn, color='blue',label='Min AIC = ' + str(aicMinIn))\n",
    "    plt.axvline(bicMinIn, color='red',label='Min BIC = ' + str(bicMinIn))\n",
    "    plt.legend(ncol=2, loc='upper left')\n",
    "    \n",
    "    \n",
    "    return AicDf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAICBIC(nCpyRdata, 2, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can not understand the ideal number of components to have in the GMM model with AIC and BIC values. BIC penalizes the components heavily for increase in components. The ideal number given by the pair is 5 for the entire training dataset (2005 - 2018).\n",
    "\n",
    "The scope of GMM is as follows:\n",
    "1. Identify the different distributions and tracing if a stable distribution for the pair of test/train datasets can be used to predict the future values of returns\n",
    "2. Based on the distribution, taking risk based decisions to reduce the market position for the stock \n",
    "3. Set trading filters based on the position, price and returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSiz = 252\n",
    "folds = len(Rdata) - 1 \n",
    "tsobj = TimeSeriesSplit(n_splits=folds, max_train_size=maxSiz)\n",
    "j = 0\n",
    "k = 0\n",
    "\n",
    "RawRow = []\n",
    "pred_df_all = []\n",
    "sym = 'SPY'\n",
    "\n",
    "\n",
    "for train_index, test_index in tsobj.split(Rdata):\n",
    "        if k < 252:\n",
    "            k = k + 1\n",
    "            continue\n",
    "\n",
    "        Rdata_train = Rdata[train_index] \n",
    "        Rdata_test = Rdata[test_index]\n",
    "#         print(Rdata_test[0])\n",
    "#         print('--------')\n",
    "#         X_test = Rdata.iloc[test_index].values.reshape(1, -1)\n",
    "#         print(X_test.ravel()[0])\n",
    "        nCpyRdata = np.asarray(Rdata_train) \n",
    "        nCpyRdata = nCpyRdata.reshape(-1,1)\n",
    "\n",
    "        nCpyRdataTest = np.asarray(Rdata_test) \n",
    "        nCpyRdataTest = nCpyRdataTest.reshape(-1,1)\n",
    "        \n",
    "        GMM = create_gmm(3,150,0)\n",
    "        GMM = GMM.fit(nCpyRdata)\n",
    "        \n",
    "        all_prob = GMM.predict_proba(nCpyRdata)\n",
    "        \n",
    "        state_prob = pd.DataFrame(all_prob,\n",
    "                            columns=['s1','s2','s3'],index=Rdata.iloc[train_index].index)\n",
    "\n",
    "        state_df = Rdata.iloc[train_index].to_frame()\n",
    "        hs_prob_df = (pd.concat([state_df,\n",
    "                                 state_prob],axis=1))\n",
    "        \n",
    "        # get state probability means and stds\n",
    "        s1_mu = hs_prob_df.query('abs(s1)>0.5')[sym].mean() \n",
    "        s2_mu = hs_prob_df.query('abs(s2)>0.5')[sym].mean() \n",
    "        s3_mu = hs_prob_df.query('abs(s3)>0.5')[sym].mean() \n",
    "#         s4_mu = hs_prob_df.query('abs(s4)>0.5')[sym].mean() \n",
    "#         s5_mu = hs_prob_df.query('abs(s5)>0.5')[sym].mean() \n",
    "\n",
    "        \n",
    "        ## Working Code \n",
    "        \n",
    "        pred_label = GMM.predict(nCpyRdata)\n",
    "        last_label = pred_label[-1]\n",
    "        last_prob = all_prob[-1][last_label]\n",
    "        pred_var = GMM.covariances_.ravel()[last_label]\n",
    "        pred_mean = GMM.means_.ravel()[last_label]\n",
    "        \n",
    "        rvs = scpSta.norm.rvs(loc=pred_mean,scale=np.sqrt(pred_var),size=1000)\n",
    "        lower_i,upper_i = scpSta.norm.interval(0.95, loc=np.mean(rvs), scale=np.std(rvs))\n",
    "        row = Rdata_test.index[0], last_label, Rdata_test[0], pred_var, pred_mean, upper_i, lower_i, last_prob, s1_mu, s2_mu, s3_mu \n",
    "                \n",
    "        ### Last step\n",
    "        RawRow.append(row)\n",
    "\n",
    "cols = ['Date','pred_label', 'Actual Return','pred_class_variance','pred_class_mean','upper_i','lower_i','last_prob','class_0_mean','class_1_mean'\n",
    "       ,'class_2_mean']\n",
    "\n",
    "pred_df_all = pd.DataFrame(RawRow,columns=cols)\n",
    "        \n",
    "pred_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CI_function(df):\n",
    "    filt = (df['lower_i'] < df['Actual Return']) & (df['Actual Return'] < df['upper_i'])\n",
    "    df['in_range'] = filt\n",
    "    return df\n",
    "\n",
    "def outlier_direction(df):\n",
    "    filt = (df['Actual Return'] > df['upper_i'])\n",
    "    df['too_high'] = filt\n",
    "    return df\n",
    "\n",
    "def labelBuy(df, thres):\n",
    "    filt = (df['too_high'] == False) & (df['in_range'] == True) & (df['last_prob'] > thres) \n",
    "    df['purchase'] =filt\n",
    "    return df\n",
    "\n",
    "pred_test = pred_df_all.pipe(CI_function).pipe(outlier_direction).pipe(labelBuy, thres=0.5)\n",
    "\n",
    "pred_test.set_index(pred_test.Date,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predPlot(df):\n",
    "    accu = len(df[df['in_range'] == True])/ len(df)\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    plt.scatter(df.index, df['Actual Return'], c=df.pred_label, s=5, cmap='autumn', label='Accuracy : ' + str(accu))\n",
    "    plt.title('Predicted Classes for Actual Returns' + ' | Accuracy : ' + str(accu))\n",
    "    plt.xlabel('Dates')\n",
    "    plt.ylabel('Log Returns')\n",
    "    \n",
    "predPlot(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_123236\\2972218522.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutlierDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutlierDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutliers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#     sns.set_xticklabels(outlierDf.index.df.year)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mplotOut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pred_test' is not defined"
     ]
    }
   ],
   "source": [
    "def plotOut(pred_test,F):\n",
    "    grp = pred_test.groupby([pd.Grouper(freq=F)])['in_range'] \n",
    "    outlierDf = (grp.count()-grp.sum()).reset_index()\n",
    "\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "    if F=='Y':\n",
    "        outlierDf.Date = outlierDf['Date'].dt.year\n",
    "    else:\n",
    "        outlierDf.Date = outlierDf['Date'].dt.to_period('M')\n",
    "    \n",
    "    outlierDf.set_index(outlierDf.Date,inplace=True)\n",
    "    outlierDf.rename(columns={'Date':F,'in_range':'Outliers'},inplace=True)\n",
    "    sns.barplot(outlierDf.index,outlierDf.Outliers)\n",
    "#     sns.set_xticklabels(outlierDf.index.df.year)\n",
    "\n",
    "plotOut(pred_test,'Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Event Study\n",
    "\n",
    "Understanding the behaviour around the prediction column for 'Buy/Sell':\n",
    "1. To test the whether for a given time T, the column prediction of 'Buy' is accurate we will plot the returns over the period of T-N and T+N. \n",
    "2. If there is significant increase in returns after the event (T) then we can assume that the predcition for 'Buy' has some weight to it. \n",
    "\n",
    "Reference: https://www.quantopian.com/posts/research-looking-for-drift-an-event-study-with-share-buybacks-announcements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_test[pred_test['purchase']==True])/len(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualAmt = prices_df['SPY']\n",
    "predAll = pd.merge(actualAmt, pred_test, left_index=True, right_index=True)\n",
    "predAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import timedelta\n",
    "\n",
    "# def get_series(data,  n_days, current_day):\n",
    "    \n",
    "#     total_date_index_length = len(data['SPY'].index)\n",
    "#     #: Find the closest date to the target date\n",
    "#     date_index = data['SPY'].index.searchsorted(current_day + timedelta(n_days))\n",
    "#     date_index_minus = data['SPY'].index.searchsorted(current_day - timedelta(n_days))\n",
    "#     #: If the closest date is too far ahead, reset to the latest date possible\n",
    "#     date_index = total_date_index_length - 1 if date_index >= total_date_index_length else date_index\n",
    "\n",
    "    \n",
    "# #     SlicData = data.iloc[current_day-timedelta(n_days):current_day+timedelta(n_days),:]\n",
    "# #     return SlicData.iloc[current_day-1:current_day+1,]\n",
    "#     return (data['SPY'].iloc[date_index] - data['SPY'].iloc[date_index_minus])/data['SPY'].iloc[date_index_minus]\n",
    "\n",
    "\n",
    "def get_close_price(data, current_date, day_number):\n",
    "    #: If we're looking at day 0 just return the indexed date\n",
    "#     if day_number == 0:\n",
    "#         return data['SPY'].ix[current_date]\n",
    "#     #: Find the close price day_number away from the current_date\n",
    "#     else:\n",
    "    #: If the close price is too far ahead, just get the last available\n",
    "    total_date_index_length = len(data['SPY'].index)\n",
    "    #: Find the closest date to the target date\n",
    "    date_index = data['SPY'].index.searchsorted(current_date + timedelta(day_number))\n",
    "    #: If the closest date is too far ahead, reset to the latest date possible\n",
    "    date_index = total_date_index_length - 1 if date_index >= total_date_index_length else date_index\n",
    "    #: Use the index to return a close price that matches\n",
    "    return data['SPY'].iloc[date_index]\n",
    "    \n",
    "def get_first_price(data, starting_point, date):\n",
    "    starting_day = date - timedelta(starting_point)\n",
    "    date_index = data['SPY'].index.searchsorted(starting_day)\n",
    "    return data['SPY'].iloc[date_index]\n",
    "\n",
    "def remove_outliers(returns, num_std_devs):\n",
    "    return returns[~((returns-returns.mean()).abs()>num_std_devs*returns.std())]\n",
    "\n",
    "def get_returns(data, starting_point, date, day_num):\n",
    "    #: Get stock prices\n",
    "    first_price = get_first_price(data, starting_point, date)\n",
    "    close_price = get_close_price(data, date, day_num)\n",
    "\n",
    "    #: Calculate returns\n",
    "    ret = (close_price - first_price)/(first_price + 0.0)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_event_study(predAll, starting_point=30):\n",
    "    \"\"\"\n",
    "    fn: wrapper for event study computations\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "        pred_df: pd.DataFrame with prediction data in it\n",
    "        starting_point: int(), days to lookbackward and forward from event\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        all_returns: pd.Series\n",
    "        all_std_devs: pd.Series\n",
    "        N: int, number of samples\n",
    "        all_returns_df: pd.DataFrame, all return series bundled\n",
    "    \"\"\"\n",
    "    data = predAll.query('purchase==True').copy()\n",
    "    #: Dictionaries that I'm going to be storing calculated data in \n",
    "    all_returns = {}\n",
    "    all_std_devs = {}\n",
    "    total_sample_size = {}\n",
    "\n",
    "    #: Create our range of day_numbers that will be used to calculate returns\n",
    "    #: Looking from -starting_point till +starting_point which creates our timeframe band\n",
    "    day_numbers = [i for i in range(-starting_point, starting_point)]\n",
    "\n",
    "    all_return_series = []\n",
    "\n",
    "    for day_num in day_numbers:\n",
    "\n",
    "        #: Reset our returns and sample size each iteration\n",
    "        returns = []\n",
    "        sample_size = 0\n",
    "\n",
    "        #: Get the return compared to t=0 \n",
    "        #for date, row in ev_data.iterrows():\n",
    "        for row in data.itertuples():\n",
    "            date = row.Index\n",
    "            #sid = row.symbol\n",
    "\n",
    "            #: Make sure that data exists for the dates\n",
    "            #if date not in data['close_price'].index or sid not in data['close_price'].columns:\n",
    "            #    continue\n",
    "\n",
    "            returns.append(get_returns(data, starting_point, date, day_num))\n",
    "            sample_size += 1\n",
    "\n",
    "        #: Drop any Nans, remove outliers, find outliers and aggregate returns and std dev\n",
    "        returns = pd.Series(returns).dropna()\n",
    "        all_return_series.append(returns)\n",
    "        all_returns[day_num] = np.average(returns)\n",
    "        all_std_devs[day_num] = np.std(returns)\n",
    "        total_sample_size[day_num] = sample_size\n",
    "\n",
    "    #: Take all the returns, stds, and sample sizes that I got and put that into a Series\n",
    "    all_returns = pd.Series(all_returns)\n",
    "    all_std_devs = pd.Series(all_std_devs)\n",
    "    N = np.average(pd.Series(total_sample_size))\n",
    "    ## combine all return series into dataframe\n",
    "    all_returns_df = pd.DataFrame(all_return_series, index=day_numbers)\n",
    "    return all_returns, all_std_devs, N, all_returns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_returns, all_std_devs, N, all_returns_df = run_event_study(predAll, starting_point=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_event_returns(all_returns, thres, starting_point=30):\n",
    "    day_numbers = [i for i in range(-starting_point, starting_point)]\n",
    "    xticks = [d for d in day_numbers if d%2 == 0]\n",
    "\n",
    "    f, ax=plt.subplots(figsize=(12,7))\n",
    "    (all_returns-all_returns.loc[0]).plot(xticks=xticks, ax=ax, \n",
    "                                          label=f\"$N={N}$\", legend=True)\n",
    "\n",
    "    plt.axhline(0, ls='--', color='red')    \n",
    "    plt.axvline(0, ls='--', color='red')\n",
    "    plt.grid(b=None, which=u'major', axis=u'y')\n",
    "    plt.title(f\"Cumulative Return from buy signals given state prob. > ${thres}$\")\n",
    "    plt.xlabel(\"Window Length (t)\")\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Cumulative Return ($r$)\");\n",
    "    \n",
    "def plot_event_returns_all_paths(all_returns_df, thres, starting_point=30):\n",
    "    day_numbers = [i for i in range(-starting_point, starting_point)]\n",
    "    xticks = [d for d in day_numbers if d%2 == 0]\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(12,7))\n",
    "\n",
    "    (all_returns_df-all_returns_df.mean(axis=1).loc[0]).plot(\n",
    "        xticks=xticks, ax=ax, color='xkcd:light gray', legend=False, alpha=0.1)\n",
    "    (all_returns_df.mean(axis=1)-all_returns_df.mean(axis=1).loc[0]).plot(\n",
    "        xticks=xticks, ax=ax, color='xkcd:midnight blue', linewidth=3,\n",
    "        label=f\"$N={N}$\", legend=True)\n",
    "    ax.set_xlim(-starting_point, starting_point)\n",
    "    ax.axhline(0, ls='--', color='red')\n",
    "    ax.axvline(0, ls='--', color='red')\n",
    "    plt.grid(b=None, which=u'major', axis=u'y')\n",
    "    plt.title(f\"Cumulative Return from buy signals given state prob. > ${thres}$ (all paths)\")\n",
    "    plt.xlabel(\"Window Length (t)\")\n",
    "    plt.ylabel(\"Cumulative Return (r)\");\n",
    "        \n",
    "\n",
    "def plot_event_sharpe_estimate(all_returns,all_std_devs, thres, starting_point=30):\n",
    "    day_numbers = [i for i in range(-starting_point, starting_point)]\n",
    "    xticks = [d for d in day_numbers if d%2 == 0]\n",
    "\n",
    "    f, ax=plt.subplots(figsize=(12,7))\n",
    "\n",
    "    mod_sharpe = ((all_returns-all_returns.loc[0]) / all_std_devs)\n",
    "    mod_sharpe.plot(xticks=xticks, ax=ax, label=f\"$N={N}$\", legend=True)\n",
    "\n",
    "    #ax.axvline(mod_sharpe.idxmax(), ls='--', color=blue)    \n",
    "    plt.axvline(0, ls='--', color='red')\n",
    "    plt.axhline(0, ls='--', color='red')\n",
    "    plt.grid(b=None, which=u'major', axis=u'y')\n",
    "    plt.title(f\"Sharpe estimate from buy signals with state prob. > ${thres}$\")\n",
    "    plt.xlabel(\"Window Length (t)\")\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"Sharpe Ratio ($\\mu / \\sigma$)\");\n",
    "    print(f'state prob. > ${thres}$ max sharpe ratio: {mod_sharpe.max():.3} at {mod_sharpe.idxmax()} days')\n",
    "    \n",
    "def plot_event_density(all_returns_df, thres):\n",
    "    (pn.ggplot(pd.melt(all_returns_df).dropna(), pn.aes(x='value'))\n",
    "     +pn.geom_density(pn.aes(y='..density..'))\n",
    "     +pn.geom_histogram(pn.aes(y='..density..', bins=10), alpha=0.2)\n",
    "     + pn.theme(axis_text_x=pn.element_text(rotation=50),\n",
    "                text=pn.element_text(size=7), figure_size=(9,6))      \n",
    "     + pn.ggtitle(f'Event study density with state prob. > ${thres}$')).draw();        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRES = 0.5\n",
    "plot_mean_event_returns(all_returns, thres=THRES)\n",
    "plot_event_returns_all_paths(all_returns_df, thres=THRES)    \n",
    "plot_event_sharpe_estimate(all_returns, all_std_devs, thres=THRES)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of the graphs show that our event is at '0' index.\n",
    "1. When the state probability is > 0.5 we can see that there is 5% returns in the next 30 days for all 'Purchase = True' predictions\n",
    "2. For all of the observations the plot is flatter in comparison\n",
    "3. Sharpe ration gives the understanding of returns given the voaltility involed. As we can see the sharpe ratio enters value range (-1.4, 0 ) till the event and then for the next 30 days it reaches the safe +0.16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allStatus = ffn.calc_stats(predAll['SPY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allStatus.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
